# NLP TERMS

# tokenizing -> form of grouping | word tokenizer, sentence tokenizer

# corpora = body of text ex : medical journals, presidential speeches, English Language

# lexicon - words and their meanings
# investor speak - regular english speak | bull / bear

from tokenizing import main as tokens
from stop_words import main as stop_words
from stemming import main as stemming
from parts_of_speech import main as parts_of_speech
from chunking import main as chunking
from chinking import main as chinking
from named_entity_recognition import main as named_entity_recognition
from lemmatizing import main as lemmatizing
from corpora import main as corpora
from wordnet import main as wordnet
from text_classification import main as text_classification
from text_classification_new_short_data import main as text_classification_new_short_data

if __name__ == '__main__':
    # tokens()
    # stop_words()
    # stemming()
    # parts_of_speech()
    # chunking()
    # chinking()
    # named_entity_recognition()
    # lemmatizing()
    # corpora()
    # wordnet()
    # text_classification()
    text_classification_new_short_data()